{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c29e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af089118",
   "metadata": {},
   "source": [
    "# Understanding Vectors and Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c73660b",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3100a040",
   "metadata": {},
   "source": [
    "Vectors are a one-dimensional array of numbers in which each number could be identified\n",
    "by its respective indices.   \n",
    "They are represented as:   \n",
    "x=[x1  \n",
    "   x2  \n",
    "   x3]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6e168",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f059bbf4",
   "metadata": {},
   "source": [
    "Matrices are an extension of arrays. They are a rectangular array of numbers wherein each\n",
    "number is identified by two indices. Like vectors, matrices are also represented using\n",
    "squared brackets, but matrices have both rows and columns, as shown in the following\n",
    "screenshot.    \n",
    "A=[x11  x12  \n",
    "     x21  x22]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667efc3",
   "metadata": {},
   "source": [
    "The following code block will give you some perspective about building vectors and\n",
    "matrices based on text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "44b26314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computers': 2, 'analyze': 1, 'text': 7, 'using': 8, 'vectors': 9, 'matrices': 5, 'process': 6, 'massive': 4, 'amounts': 0, 'data': 3}\n",
      "[[0 1 1 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 1]\n",
      " [1 0 1 1 1 0 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "X = (\"Computers can analyze text\", \"They do it using vectors and matrices\", \"Computers can process massive amounts of text data\")\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(X_vec.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb27f20e",
   "metadata": {},
   "source": [
    "The following output block from the previous code block shows a matrix, wherein each\n",
    "row corresponds to the document being imported in the same order and each column\n",
    "corresponds to a unique token whose ordering can be obtained using\n",
    "the .vocabulary_ function of the CountVectorizer class:  \n",
    "Once text data is converted into a matrix, we can apply any matrix operation to it (vectormatrix multiplication, matrix-matrix multiplication, transpose, and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d8611",
   "metadata": {},
   "source": [
    "# Exploring the Bag-of-Words architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e438c6",
   "metadata": {},
   "source": [
    "### Take in a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7cbda5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"We are reading about Natural Language Processing Here\",\n",
    "            \"Natural Language Processing making computers comprehend language data\",\n",
    "            \"The field of Natural Language Processing is evolving everyday\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97307ea",
   "metadata": {},
   "source": [
    "### Create a Pandas Series of the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11fa1767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    We are reading about Natural Language Processi...\n",
       "1    Natural Language Processing making computers c...\n",
       "2    The field of Natural Language Processing is ev...\n",
       "dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.Series(sentences)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65b435",
   "metadata": {},
   "source": [
    "### Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c16bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(corpus, keep_list):\n",
    "    '''\n",
    "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
    "    \n",
    "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
    "            even after the cleaning process\n",
    "    \n",
    "    Output : Returns the cleaned text corpus\n",
    "    \n",
    "    '''\n",
    "    cleaned_corpus = pd.Series()\n",
    "    for row in corpus:\n",
    "        qs = []\n",
    "        for word in row.split():\n",
    "            if word not in keep_list:\n",
    "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
    "                p1 = p1.lower()\n",
    "                qs.append(p1)\n",
    "            else : qs.append(word)\n",
    "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b612ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_removal(corpus):\n",
    "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for word in wh_words:\n",
    "        stop.remove(word)\n",
    "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
    "    return corpus\n",
    "\n",
    "def lemmatize(corpus):\n",
    "    lem = WordNetLemmatizer()\n",
    "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
    "    return corpus\n",
    "\n",
    "def stem(corpus, stem_type = None):\n",
    "    if stem_type == 'snowball':\n",
    "        stemmer = SnowballStemmer(language = 'english')\n",
    "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "    else :\n",
    "        stemmer = PorterStemmer()\n",
    "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e085454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
    "    '''\n",
    "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
    "    \n",
    "    Input : \n",
    "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
    "    'keep_list' - List of words to be retained during cleaning process\n",
    "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should \n",
    "                                                                  be performed or not\n",
    "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
    "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
    "    \n",
    "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
    "    \n",
    "    Output : Returns the processed text corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if cleaning == True:\n",
    "        corpus = text_clean(corpus, keep_list)\n",
    "    \n",
    "    if remove_stopwords == True:\n",
    "        corpus = stopwords_removal(corpus)\n",
    "    else :\n",
    "        corpus = [[x for x in x.split()] for x in corpus]\n",
    "    \n",
    "    if lemmatization == True:\n",
    "        corpus = lemmatize(corpus)\n",
    "        \n",
    "        \n",
    "    if stemming == True:\n",
    "        corpus = stem(corpus, stem_type)\n",
    "    \n",
    "    corpus = [' '.join(x) for x in corpus]        \n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0c643a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dot_words = ['U.S.', 'Mr.', 'Mrs.', 'D.C.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "307c37fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3088\\2650312229.py:11: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  cleaned_corpus = pd.Series()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3088\\2650312229.py:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3088\\2650312229.py:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3088\\2650312229.py:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['read natural language process',\n",
       " 'natural language process make computers comprehend language data',\n",
       " 'field natural language process evolve everyday']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing with Lemmatization here\n",
    "preprocessed_corpus = preprocess(corpus, keep_list = common_dot_words, stemming = False, stem_type = None, lemmatization = True, remove_stopwords = True)\n",
    "preprocessed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e4b54",
   "metadata": {},
   "source": [
    "### Building the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "65109c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['field', 'language', 'read', 'evolve', 'make', 'process', 'comprehend', 'natural', 'computers', 'everyday', 'data']\n"
     ]
    }
   ],
   "source": [
    "set_of_words = set()\n",
    "for sentence in preprocessed_corpus:\n",
    "    for word in sentence.split():\n",
    "        set_of_words.add(word)\n",
    "vocab = list(set_of_words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd4488",
   "metadata": {},
   "source": [
    "### Fetching the position of each word in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53b5863c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'field': 0, 'language': 1, 'read': 2, 'evolve': 3, 'make': 4, 'process': 5, 'comprehend': 6, 'natural': 7, 'computers': 8, 'everyday': 9, 'data': 10}\n"
     ]
    }
   ],
   "source": [
    "position = {}\n",
    "for i, token in enumerate(vocab):\n",
    "    position[token] = i\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f52f70",
   "metadata": {},
   "source": [
    "### Creating a matrix to hold the Bag of Words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3f66d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix = np.zeros((len(preprocessed_corpus), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bd0d9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, preprocessed_sentence in enumerate(preprocessed_corpus):\n",
    "    for token in preprocessed_sentence.split():   \n",
    "        bow_matrix[i][position[token]] = bow_matrix[i][position[token]] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c358e",
   "metadata": {},
   "source": [
    "### Let's look at our Bag of Words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f56ce258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
       "       [0., 2., 0., 0., 1., 1., 1., 1., 1., 0., 1.],\n",
       "       [1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350779c",
   "metadata": {},
   "source": [
    "## Inference  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7682aeb",
   "metadata": {},
   "source": [
    "Taking example of column 2 in the bow_matrix, the values are 1, 2 and 1 respectively.  \n",
    "\n",
    "Column 2 caters to index 2 corresponding to the word language.  \n",
    "\n",
    "language occurs once, twice and again once in the the sentences 1, 2 and 3 respectively.  \n",
    "  \n",
    "Hope that provides you insights into how the Bag of Words model works.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf241b6",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency based Vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e1d4f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0964c5",
   "metadata": {},
   "source": [
    "## Building a corpus of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "edfb3332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    We are reading about Natural Language Processi...\n",
       "1    Natural Language Processing making computers c...\n",
       "2    The field of Natural Language Processing is ev...\n",
       "dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"We are reading about Natural Language Processing Here\",\n",
    "            \"Natural Language Processing making computers comprehend language data\",\n",
    "            \"The field of Natural Language Processing is evolving everyday\"]\n",
    "corpus = pd.Series(sentences)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea0b6f3",
   "metadata": {},
   "source": [
    "## Data preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2e35e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3088\\2650312229.py:11: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  cleaned_corpus = pd.Series()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3088\\2650312229.py:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3088\\2650312229.py:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3088\\2650312229.py:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['read natural language process',\n",
       " 'natural language process make computers comprehend language data',\n",
       " 'field natural language process evolve everyday']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing with Lemmatization here\n",
    "preprocessed_corpus = preprocess(corpus, keep_list = [], stemming = False, stem_type = None,\n",
    "                                lemmatization = True, remove_stopwords = True)\n",
    "preprocessed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7051f8",
   "metadata": {},
   "source": [
    "## Tf-IdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3bd448aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d890801",
   "metadata": {},
   "source": [
    "### Let's what features were obtained and the corresponding TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3ab55468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
      " 'make' 'natural' 'process' 'read']\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.41285857 0.         0.41285857 0.41285857 0.69903033]\n",
      " [0.40512186 0.40512186 0.40512186 0.         0.         0.\n",
      "  0.478543   0.40512186 0.2392715  0.2392715  0.        ]\n",
      " [0.         0.         0.         0.49711994 0.49711994 0.49711994\n",
      "  0.29360705 0.         0.29360705 0.29360705 0.        ]]\n",
      "\n",
      "The shape of the TF-IDF matrix is:  (3, 11)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(tf_idf_matrix.toarray())\n",
    "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0350880",
   "metadata": {},
   "source": [
    "### Changing the norm to l1, default option is l2 which was used above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c97e2",
   "metadata": {},
   "source": [
    "Each output row will have unit norm, which can be one of  \n",
    "l2: Sum of squares of vector elements is 1.  \n",
    "l1: Sum of absolute values of vector elemen  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f136fb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
      " 'make' 'natural' 'process' 'read']\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.21307663 0.         0.21307663 0.21307663 0.3607701 ]\n",
      " [0.1571718  0.1571718  0.1571718  0.         0.         0.\n",
      "  0.1856564  0.1571718  0.0928282  0.0928282  0.        ]\n",
      " [0.         0.         0.         0.2095624  0.2095624  0.2095624\n",
      "  0.12377093 0.         0.12377093 0.12377093 0.        ]]\n",
      "\n",
      "The shape of the TF-IDF matrix is:  (3, 11)\n"
     ]
    }
   ],
   "source": [
    "vectorizer_l1_norm = TfidfVectorizer(norm=\"l1\")\n",
    "tf_idf_matrix_l1_norm = vectorizer_l1_norm.fit_transform(preprocessed_corpus)\n",
    "print(vectorizer_l1_norm.get_feature_names_out())\n",
    "print(tf_idf_matrix_l1_norm.toarray())\n",
    "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix_l1_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba68942",
   "metadata": {},
   "source": [
    "## N-grams and Max features with TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eda339b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language' 'language process' 'natural' 'natural language'\n",
      " 'natural language process' 'process']\n",
      "[[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]\n",
      " [0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
      " [0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]]\n",
      "\n",
      "The shape of the TF-IDF matrix is:  (3, 6)\n"
     ]
    }
   ],
   "source": [
    "vectorizer_n_gram_max_features = TfidfVectorizer(norm=\"l2\", analyzer='word', ngram_range=(1,3), max_features = 6)\n",
    "tf_idf_matrix_n_gram_max_features = vectorizer_n_gram_max_features.fit_transform(preprocessed_corpus)\n",
    "print(vectorizer_n_gram_max_features.get_feature_names_out())\n",
    "print(tf_idf_matrix_n_gram_max_features.toarray())\n",
    "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix_n_gram_max_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b234ece",
   "metadata": {},
   "source": [
    "# Cosine Similarity Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f18db3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "    return np.dot(vector1, vector2) / (np.sqrt(np.sum(vector1**2)) * np.sqrt(np.sum(vector2**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf39c22",
   "metadata": {},
   "source": [
    "## CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f69c88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "42e6793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
      " 'make' 'natural' 'process' 'read']\n",
      "[[0 0 0 0 0 0 1 0 1 1 1]\n",
      " [1 1 1 0 0 0 2 1 1 1 0]\n",
      " [0 0 0 1 1 1 1 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4fd803",
   "metadata": {},
   "source": [
    "## Cosine similarity between the document vectors built using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f9a24dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the documents  0 and 1 is:  0.6324555320336759\n",
      "The cosine similarity between the documents  0 and 2 is:  0.6123724356957946\n",
      "The cosine similarity between the documents  1 and 2 is:  0.5163977794943223\n"
     ]
    }
   ],
   "source": [
    "for i in range(bow_matrix.shape[0]):\n",
    "    for j in range(i + 1, bow_matrix.shape[0]):\n",
    "        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \",\n",
    "              cosine_similarity(bow_matrix.toarray()[i], bow_matrix.toarray()[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b05158e",
   "metadata": {},
   "source": [
    "## TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4e0909a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
      " 'make' 'natural' 'process' 'read']\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.41285857 0.         0.41285857 0.41285857 0.69903033]\n",
      " [0.40512186 0.40512186 0.40512186 0.         0.         0.\n",
      "  0.478543   0.40512186 0.2392715  0.2392715  0.        ]\n",
      " [0.         0.         0.         0.49711994 0.49711994 0.49711994\n",
      "  0.29360705 0.         0.29360705 0.29360705 0.        ]]\n",
      "\n",
      "The shape of the TF-IDF matrix is:  (3, 11)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(tf_idf_matrix.toarray())\n",
    "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2287c006",
   "metadata": {},
   "source": [
    "## Cosine similarity between the document vectors built using TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ec423810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the documents  0 and 1 is:  0.39514115766749125\n",
      "The cosine similarity between the documents  0 and 2 is:  0.36365455673761865\n",
      "The cosine similarity between the documents  1 and 2 is:  0.2810071916500233\n"
     ]
    }
   ],
   "source": [
    "for i in range(tf_idf_matrix.shape[0]):\n",
    "    for j in range(i + 1, tf_idf_matrix.shape[0]):\n",
    "        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \",\n",
    "              cosine_similarity(tf_idf_matrix.toarray()[i], tf_idf_matrix.toarray()[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c366e",
   "metadata": {},
   "source": [
    "# One Hot Vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd7aa8",
   "metadata": {},
   "source": [
    "# We take only 1 sentence as input here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c5e6485a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    We are reading about Natural Language Processi...\n",
       "dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\"We are reading about Natural Language Processing Here\"]\n",
    "corpus = pd.Series(sentence)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6ead28a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3088\\2650312229.py:11: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  cleaned_corpus = pd.Series()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3088\\2650312229.py:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['read natural language process']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing with Lemmatization here\n",
    "preprocessed_corpus = preprocess(corpus, keep_list = [], stemming = False, stem_type = None,\n",
    "                                lemmatization = True, remove_stopwords = True)\n",
    "preprocessed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a1cf6",
   "metadata": {},
   "source": [
    "## Building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cfc2e456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['read', 'process', 'natural', 'language']\n"
     ]
    }
   ],
   "source": [
    "set_of_words = set()\n",
    "for word in preprocessed_corpus[0].split():\n",
    "    set_of_words.add(word)\n",
    "vocab = list(set_of_words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fbab52",
   "metadata": {},
   "source": [
    "## Fetching the position of each word in the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f386eda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'read': 0, 'process': 1, 'natural': 2, 'language': 3}\n"
     ]
    }
   ],
   "source": [
    "position = {}\n",
    "for i, token in enumerate(vocab):\n",
    "    position[token] = i\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd4db2",
   "metadata": {},
   "source": [
    "## Instantiating the one hot matrix\n",
    "### Note here every row in the matrix corresponds to the One Hot vector for an individual term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b5df3a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_matrix = np.zeros((len(preprocessed_corpus[0].split()), len(vocab)))\n",
    "one_hot_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28688d53",
   "metadata": {},
   "source": [
    "## Building One Hot Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2c50be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(preprocessed_corpus[0].split()):\n",
    "    one_hot_matrix[i][position[token]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa64a9",
   "metadata": {},
   "source": [
    "## Visualizing the One Hot Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "96d4d981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e4b18",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea70abfa",
   "metadata": {},
   "source": [
    "The first row corresponds to the One Hot vector of read, second for natural,\n",
    "third for language and the final one for process based on their respective indices in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf76b1d",
   "metadata": {},
   "source": [
    "# Building a basic chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ee6bf789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#loading questions and answers in separate lists\n",
    "import ast \n",
    "questions = []\n",
    "answers = [] \n",
    "with open('qa_Electronics.json','r') as f:\n",
    "    for line in f:\n",
    "        data = ast.literal_eval(line)\n",
    "        questions.append(data['question'].lower())\n",
    "        answers.append(data['answer'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de633e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text and convert data in matrix format\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_vec = vectorizer.fit_transform(questions)\n",
    "# Transform data by applying term frequency inverse document frequency (TF-IDF) \n",
    "tfidf = TfidfTransformer() #by default applies \"l2\" normalization\n",
    "X_tfidf = tfidf.fit_transform(X_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eb481690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation(im):\n",
    "    global tfidf, answers, X_tfidf\n",
    "    Y_vec = vectorizer.transform(im)\n",
    "    Y_tfidf = tfidf.fit_transform(Y_vec)\n",
    "    cos_sim = np.rad2deg(np.arccos(max(cosine_similarity(Y_tfidf, X_tfidf)[0])))\n",
    "    if cos_sim > 60 :\n",
    "        return \"sorry, I did not quite understand that\"\n",
    "    else:\n",
    "        return answers[np.argmax(cosine_similarity(Y_tfidf, X_tfidf)[0])]\n",
    "\n",
    "def main():\n",
    "    usr = input(\"Please enter your username: \")\n",
    "    print(\"support: Hi, welcome to Q&A support. How can I help you?\")\n",
    "    while True:\n",
    "        im = input(\"{}: \".format(usr))\n",
    "        if im.lower() == 'bye':\n",
    "            print(\"Q&A support: bye!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Q&A support: \"+conversation([im]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "77ddc83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your username: aditya\n",
      "support: Hi, welcome to Q&A support. How can I help you?\n",
      "aditya: tell me about yourself\n",
      "Q&A support: germany\n",
      "aditya: how to restart my laptop\n",
      "Q&A support: hi, you may get you laptop in 3 to 5 business day depending on you location. thanks for you interest. tech mark.\n",
      "aditya: how are you\n",
      "Q&A support: sorry, I did not quite understand that\n",
      "aditya: how can you help me?\n",
      "Q&A support: i've had the same problem since i plugged it in. a small buzzing sound that gets louder. if i tap the earpiece where the mic arm is attached it goes away briefly. i sent that pair back, but the new set does the exact same thing. any help would be appreciated.\n",
      "aditya: very good\n",
      "Q&A support: yes\n",
      "aditya: comm'n lets go\n",
      "Q&A support: sorry, I did not quite understand that\n",
      "aditya: what are you?\n",
      "Q&A support: sorry, I did not quite understand that\n",
      "aditya: okay bye\n",
      "Q&A support: i would have to agree with samuel because most people can just use their iphone or droid phones to shoot great video for example. we have a droid x that has a generous 4.5\" screen and takes hd video & sound just fine for the everyday user. videos can now be uploaded directly to youtube without having to convert them either (had to do this a year ago for quality)! i think little pocket-cams will become obsolete by 2012 (along with the world!!! lol).\n",
      "aditya: bye\n",
      "Q&A support: bye!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1kernel",
   "language": "python",
   "name": "venv1kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
